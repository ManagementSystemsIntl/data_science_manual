[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MSI data science manual",
    "section": "",
    "text": "Preface\nThis manual introduces a variety of analytical approaches MSI adopts in order to learn, teach, and meet its client deliverables."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "We used to do data science in spreadsheets, while a more select realm of demi-gods used expensive statistical analysis packages or wrote code. Now we have access to open source software that puts immense computing power in the hands of the people. The easy accessibility of such power is both a blessing and a curse. This manual seeks to bestow the blessings while avoiding the curse.\nThis manual is already out of date. Tomorrow, we will be able to tell our AI assistants what we want, and the AI instance will provide it to us through some opaque combination of computation and creation. Aspiring analysts are still encouraged to learn this manual as a way to welcome our new overlords."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "First and foremost, MSI is client driven. We provide what is asked for, following client guidance. Within this framework, we use a variety of analytical approaches and tools that follow best practice while satisfying the guidance we are under.\nThis manual provides an idealized approach of a data analysis. MSI is agnostic about the tools used to conduct an analysis, but the majority of explanation and examples are provided in the R programming language.\nIn unit one, we introduce R, explain how it works, and provide guidance as to how to get set up and start analyzing.\nIn unit two, we go through the steps of a data analysis.\nIn unit three, we review different ways to report your analyses.\nThroughout the journey, we will provide examples of MSI analyses that were used for internal learning, or to generate a client deliverable.\nLet’s start!"
  },
  {
    "objectID": "How R works.html#basic-use",
    "href": "How R works.html#basic-use",
    "title": "How R works",
    "section": "Basic use",
    "text": "Basic use\nIn R, you create objects and then use those objects for your analysis. Objects are made up of whatever you assign them to. They can be a direct value, a file, a graphic, etc. Here’s an example:\n\na &lt;- 5\n\nWe have assigned the object, a, the value of 5. The assignment operator &lt;- is what tells R to assign the value of 5 to a.\nNow we can use the object a. As in a + a. We use the # to annotate our code for human readers. R will not compute any text to the right of a #. Annotating code is very helpful for code review and for remembering what you were doing when you open up a script that you have not worked on for 6 months.\n\n# Assign a the value of 5\na &lt;- 5\n\n# Add a + a (or 5 +5)\na+a\n\n[1] 10\n\n\nNotice that R understands to output the value of a+a without any additional instructions. Or, you could store the value of a + a as a new object.\n\na &lt;- 5\n\n# Assign the value of a + a to b\nb &lt;- a + a\n\n#print value of b\nb\n\n[1] 10"
  },
  {
    "objectID": "How R works.html#data-structures",
    "href": "How R works.html#data-structures",
    "title": "How R works",
    "section": "Data Structures",
    "text": "Data Structures\nThe primary data structures in R are vectors, matrices, lists, and data frames. They all basically begin as a vector. The idea here is not to master what the data structures are, but to understand how R handles each one as it will affect more advanced coding operations. Knowledge of data structures is also helpful when debugging code because error messages will reference the different data structures.\nNaturally, we will start with the most “atomic” of the data structures, the (atomic) vector.\n\nVectors\nA vector is the most basic data structure in R. A vector can only contain a single data type. It can be any of logical, integer, double, character, complex or raw, but it cannot mix and match types.\nHere’s a vector\n\n# Create vectors\nvector &lt;- 10\nvector1 &lt;- c(10, 14, 27, 99)\nvector2 &lt;- c(\"purple\", \"blue\", \"red\")\n\n# Print the value of each vector \nvector\n\n[1] 10\n\nvector1\n\n[1] 10 14 27 99\n\nvector2\n\n[1] \"purple\" \"blue\"   \"red\""
  },
  {
    "objectID": "How R works.html#matrices",
    "href": "How R works.html#matrices",
    "title": "How R works",
    "section": "Matrices",
    "text": "Matrices\nA matrix is a vector with dimensions - it has rows and columns. As with a vector, the elements of a matrix must be of the same data type. Here are a few examples.\n\n# Create a 2 x 2 matrix with the numbers 1 through 4\nm &lt;- matrix(1:4, nrow = 2, ncol = 2) \n\n# Note that the matrix is filled column-wise. (e.g., it completes # the left column with 1 and 2 before moving to the right column \n# and entering 3 and 4 \nm\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4"
  },
  {
    "objectID": "How R works.html#lists",
    "href": "How R works.html#lists",
    "title": "How R works",
    "section": "Lists",
    "text": "Lists\nA list is a vector that can have multiple data types. You can call class() on any object in R to display the type of object that it is.\n\n# Make a list a\na &lt;- list(10, \"red\", 74, \"blue\")\n\n# What is the class, or type, of a?\nclass(a)\n\n[1] \"list\""
  },
  {
    "objectID": "How R works.html#dataframes",
    "href": "How R works.html#dataframes",
    "title": "How R works",
    "section": "Dataframes",
    "text": "Dataframes\nYou can think of a dataframe as your Excel Spreadheet. At MSI, this is the most common form of dataset. We read a .xlsx or .csv file into R, and we get a dataframe. At its core, a dataframe is a list of lists where each list (column) is the same length (i.e., it is a “rectangular list”). A data frame can contain many types of data because it is a collection of lists, and lists, as you remember, can consist of multiple data types.\n\n# Create a dataframe called df\n\ndf &lt;- data.frame(a = c(10,20,30,40)\n                 , b = c('book', 'pen', 'textbook', 'pencil_case')\n                 , c = c(TRUE,FALSE,TRUE,FALSE)\n                 , d = c(TRUE,FALSE,TRUE,FALSE))\n\n# Print df\ndf\n\n   a           b     c     d\n1 10        book  TRUE  TRUE\n2 20         pen FALSE FALSE\n3 30    textbook  TRUE  TRUE\n4 40 pencil_case FALSE FALSE\n\n\nNow that we have a dataframe, we want to look at some of its details using glimpse().\n\n# Create a dataframe called df\n\ndf &lt;- data.frame(a = c(10,20,30,40)\n                 , b = c('book', 'pen', 'textbook', 'pencil_case')\n                 , c = c(TRUE,FALSE,TRUE,FALSE)\n                 , d = c(TRUE,FALSE,TRUE,FALSE))\n\n# Look at structure of df\ndplyr::glimpse(df)\n\nRows: 4\nColumns: 4\n$ a &lt;dbl&gt; 10, 20, 30, 40\n$ b &lt;chr&gt; \"book\", \"pen\", \"textbook\", \"pencil_case\"\n$ c &lt;lgl&gt; TRUE, FALSE, TRUE, FALSE\n$ d &lt;lgl&gt; TRUE, FALSE, TRUE, FALSE"
  },
  {
    "objectID": "Data exploration.html",
    "href": "Data exploration.html",
    "title": "Data exploration",
    "section": "",
    "text": "Introduction\nIn a well-defined study, exploratory analysis may be largely unnecessary. Consider a scenario where the client has identified questions / hypotheses of interest and the additional variables that may predict or mediate the identified outcomes. An activity partner such as MSI was provided the time and resources to consult with stakeholders, scope out the target environment to identify and map the data generating process, and develop an inferential design by which the collected data and analytical routines would address the questions posed by the client. In this scenario, exploratory analysis may be entirely eliminated, and the time that may be spent on exploration is shifted to sensitivity analysis of the pre-identified analytical routines.\nOn the other hand, consider a scenario where data has been collected for one purpose, and later realized to have possible use with other, not yet identified, purposes. In this case, the initial analysis is entirely exploratory."
  },
  {
    "objectID": "Data exploration.html#data-visualization-tactics",
    "href": "Data exploration.html#data-visualization-tactics",
    "title": "Data exploration",
    "section": "Data visualization tactics",
    "text": "Data visualization tactics\n\ngeomtextpath\nThere are common situations where MSI is tasked with ongoing data collection and evaluation of a client activity. For example, the MENA MELS activity (2020-2024) was tasked with ongoing monitoring and evaluation of the Middle East Partnership for Peace Activity (MEPPA). MEPPA comprised grants to several local partners organized around the common motif of building bonds between different demographic groups. The primary outcome of interest was whether or not a participant in the grant activity reported a perceived increase in understanding the political, social, and economic situation and viewpoints of another group. Data were collected on a rolling basis across several implementing partners, across baseline and endline. That data are summarized as follows:\n\ndf1 &lt;- read_csv(\"data/short demo series/meppa response items.csv\",\n                show_col_types=F)\ndf1 %&gt;%\n  flextable() %&gt;% \n  autofit()\n\n\nitemresponselabendlinenpercPolitical views1Basic understanding0220.286Political views2Fair understanding0380.494Political views3High understanding0170.221Political views1Basic understanding1180.173Political views2Fair understanding1500.481Political views3High understanding1360.346Social views1Basic understanding0300.390Social views2Fair understanding0250.325Social views3High understanding0220.286Social views1Basic understanding1150.147Social views2Fair understanding1520.510Social views3High understanding1350.343Economic views1Basic understanding0320.416Economic views2Fair understanding0300.390Economic views3High understanding0150.195Economic views1Basic understanding1200.196Economic views2Fair understanding1550.539Economic views3High understanding1270.265\n\n\nFor purposes of client reporting, there are three ordinal responses across baseline and endline, for each of three types of viewpoint. There is insufficient data to conduct inferential tests at this level of granularity, but this data may be visualized descriptively.\nThe geomtextpath package offers the functionality to directly label line-based plots with text that is able to follow a curved path. Simply replace ‘geom_line’ with ‘geom_textpath’ and assign the variable to use as the label. The following figures illustrate.\n\npol &lt;- ggplot(filter(df1, item==\"Political views\"), aes(endline, perc, color=as.factor(response))) + \n  geom_textpath(aes(label=lab),\n                size=4) +\n  geom_label(aes(label=paste(round(perc*100,0), \"%\", sep=\"\")),\n             size=4,\n             label.padding = unit(.14, \"lines\")) +\n  scale_color_viridis_d(option=\"D\") +\n  scale_x_continuous(limits=c(-.1, 1.1),\n                     breaks=c(0,1),\n                     labels=c(\"Baseline\",\"Endline\")) +\n  scale_y_continuous(labels=percent_format(accuracy=1)) +\n  theme(legend.position=\"none\",\n        axis.text.y=element_blank()) +\n  labs(x=\"\",\n       y=\"\",\n       title=\"Political situation\") \n\npol\n\n\n\n\n\nsoc &lt;- ggplot(filter(df1, item==\"Social views\"), aes(endline, perc, color=as.factor(response))) + \n  geom_textpath(aes(label=lab),\n                size=4) +\n  geom_label(aes(label=paste(round(perc*100,0), \"%\", sep=\"\")),\n             size=4,\n             label.padding = unit(.14, \"lines\")) +\n  scale_color_viridis_d(option=\"D\") +\n  scale_x_continuous(limits=c(-.1, 1.1),\n                     breaks=c(0,1),\n                     labels=c(\"Baseline\",\"Endline\")) +\n  scale_y_continuous(labels=percent_format(accuracy=1)) +\n  theme(legend.position=\"none\",\n        axis.text.y=element_blank()) +\n  labs(x=\"\",\n       y=\"\",\n       title=\"Social situation\") \n\nsoc\n\n\n\n\n\nec &lt;- ggplot(filter(df1, item==\"Economic views\"), aes(endline, perc, color=as.factor(response))) + \n  geom_textpath(aes(label=lab),\n                size=4) +\n  geom_label(aes(label=paste(round(perc*100,0), \"%\", sep=\"\")),\n             size=4,\n             label.padding = unit(.14, \"lines\")) +\n  scale_color_viridis_d(option=\"D\") +\n  scale_x_continuous(limits=c(-.1, 1.1),\n                     breaks=c(0,1),\n                     labels=c(\"Baseline\",\"Endline\")) +\n  scale_y_continuous(labels=percent_format(accuracy=1)) +\n  theme(legend.position=\"none\",\n        axis.text.y=element_blank()) +\n  labs(x=\"\",\n       y=\"\",\n       title=\"Economic situation\") \n\nec\n\n\n\n\nGiven that the three types of understanding of others’ situation are highly correlated, it makes sense to present these measures compactly as aspects of a deeper underlying construct. The patchwork library allows multiple ggplots to be assembled together as a single plot. The following figure illustrates.\n\npol + soc + ec + \n  plot_annotation(title=\"How well do you understand the situation of others?\")\n\n\n\n\nA final use of presenting the data more compactly is to collapse the ordinal responses to binary, and collect the three measures as lines in a single plot. The following data captures each type of understanding as either fair or high understanding as one category, and basic understanding as the other category.\n\ndat &lt;- read_csv(\"data/short demo series/meppa item ladder.csv\",\n                show_col_types=F)\n\ndat_flx &lt;- dat %&gt;%\n  flextable() %&gt;%\n  autofit() \n\ndat_flx\n\n\nendlinenpercitem0550.714Political situation1860.827Political situation0470.610Social situation1870.853Social situation0450.584Economic situation1820.804Economic situation\n\n\nWith this simplified data summary, the trendline for each type of understanding may now be collected in a single plot.\n\nggplot(dat, aes(endline, perc, color=as.factor(item))) + \n  geom_textpath(aes(label=item),\n                size=4) +\n  geom_label(aes(label=paste(round(perc*100,0), \"%\", sep=\"\")),\n             size=4,\n             label.padding = unit(.14, \"lines\")) +\n  scale_color_viridis_d(option=\"D\") +\n  scale_x_continuous(limits=c(-.1, 1.1),\n                     breaks=c(0,1),\n                     labels=c(\"Baseline\",\"Endline\")) +\n  scale_y_continuous(labels=percent_format(accuracy=1),\n                     breaks=c(.5,1),\n                     sec.axis=dup_axis(breaks=c(.804,.827,.853),\n                                       labels=c(\"+22\",\"+12\",\"+24\"))) +\n  theme(legend.position=\"none\",\n        axis.text.y.left=element_blank()) +\n  labs(x=\"\",\n       y=\"\",\n       caption=\"Proportion reporting fair or high\\nunderstanding of others' situation\")\n\n\n\n\nNote further the use of secondary axis breaks to illustrate the change score for each trendline from baseline to endline.\nThe R computing language allows for several ways to customize the use of labels in statistical or descriptive graphics. This short demo has illustrated MSI’s use of the geomtextpath package to place labels directly along the line or curve of a plot. This illustration used only straight lines between two points in time. For additional use cases of the geomtextpath package, see the package vignette.\n\n\nMapping\nMuch of our data is collected from surveys and has geographic coordinates associated with a point of collection, a house, or a city, etc. It can be useful to generate a map to get a sense of where the data is coming from. Mapping is a part of MSI’s exploratory data analysis process and is also used in developing a sampling plan and in producing high quality data vizualizations for clients. This section provides a brief introduction into mapping in R.\nThe most commonly used packages to handle spatial data are sf for vectors, terra for vectors and rasters, and raster for rasters. To visualize the data, two frequently used packages are tmap and ggplot2 packages.\nTo get started, we need to load our packages.\n\n#run this line first if you have never used these packages before\n#install.packages(c(\"tidyverse\", \"sf\", \"tmap\", \"readr\", \"here\"))\n\nlibrary(tidyverse) #install the core tidyverse packages including ggplot2\nlibrary(sf) #provides tools to work with vector data \nlibrary(tmap) #for visualizing spatial data\nlibrary(readr) #functions for reading external datasets \nlibrary(here) #to better locate files not in working directory\nlibrary(geodata) #to download administrative boundaries"
  },
  {
    "objectID": "Data exploration.html#read-in-the-data",
    "href": "Data exploration.html#read-in-the-data",
    "title": "Data exploration",
    "section": "Read in the data",
    "text": "Read in the data\n\n#It is a csv file so I use the read_csv function and provide the file path\ncities &lt;- read_csv(here::here(\"../methods corner/Map demo/data/Madagascar_Cities.csv\")\n                   , show_col_types = FALSE)\n\n#Observe the first few rows of data\nDT::datatable(head(cities))\n\n\n\n\n\n\nNow, for the administrative boundaries. Each of these are being read in using gadm() from the geodata package and then converted to an sf object with the st_as_sf() function. In the example, we download only the country boundary, but if we wanted regions or departments, we would simply change the level argument inside the gadm() function call to a 1 or a 2."
  },
  {
    "objectID": "Data exploration.html#country-boundary",
    "href": "Data exploration.html#country-boundary",
    "title": "Data exploration",
    "section": "Country boundary",
    "text": "Country boundary\n\n#This is only the country boundary. \nmdg &lt;- geodata::gadm(country = \"MDG\"\n                  , level = 0\n                  , path = tempdir()) |&gt;\n  st_as_sf()"
  },
  {
    "objectID": "Data exploration.html#convert-the-cities-to-an-sf-object",
    "href": "Data exploration.html#convert-the-cities-to-an-sf-object",
    "title": "Data exploration",
    "section": "Convert the cities to an sf object",
    "text": "Convert the cities to an sf object\nRemember that the cities object is a standard .csv with longitude and latitude columns, but it is not yet recognized as an sf object that has geographic properties. Here is how to convert it to an sf object with a single geometry column and a crs.\n\ncities_sf &lt;- cities |&gt;\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\")\n           , crs = 4326)\n\n#observe the first few rows of data\nDT::datatable(head(cities_sf))"
  },
  {
    "objectID": "Data exploration.html#make-the-map",
    "href": "Data exploration.html#make-the-map",
    "title": "Data exploration",
    "section": "Make the map",
    "text": "Make the map\nThe following code chunks and tabs walk through the process of making and improving a map in both tmap and ggplot2. In the example, cities are what we are plotting, but we could be plotting any variable of a dataset.\n\ntmapggplot2\n\n\n\ntmap_mode(\"plot\") +\n  tm_shape(mdg) +\n  tm_polygons() + #for only the borders, use tm_borders()\n  tm_shape(cities_sf) +\n  tm_dots(size = .25, col = \"red\")\n\n\n\n\n\n\n\nggplot2::ggplot(mdg) +\n  geom_sf() +\n  geom_sf(data = cities_sf, color = \"red\")"
  },
  {
    "objectID": "Data exploration.html#make-the-map-better",
    "href": "Data exploration.html#make-the-map-better",
    "title": "Data exploration",
    "section": "Make the map better",
    "text": "Make the map better\n\ntmapggplot2\n\n\n\n#the city names are long so we have to \n# make a bigger window to fit them. This isn't part of the normal process\n#make an object with the current bounding box\nbbox_new &lt;- st_bbox(mdg)\n\n#calculate the x and y ranges of the bbox\nxrange &lt;- bbox_new$xmax - bbox_new$xmin # range of x values\nyrange &lt;- bbox_new$ymax - bbox_new$ymin # range of y values\n\n#provide the new values for the 4 corners of the bbox\n  bbox_new[1] &lt;- bbox_new[1] - (0.7 * xrange) # xmin - left\n  bbox_new[3] &lt;- bbox_new[3] + (0.75 * xrange) # xmax - right\n  bbox_new[2] &lt;- bbox_new[2] - (0.1 * yrange) # ymin - bottom\n  bbox_new[4] &lt;- bbox_new[4] + (0.1 * yrange) # ymax - top\n\n#convert the bbox to a sf collection (sfc)\nbbox_new &lt;- bbox_new |&gt;  # take the bounding box ...\n  st_as_sfc() # ... and make it a sf polygon\n\n#now plot the map\ntmap_mode(\"plot\") +\n  tm_shape(mdg, bbox = bbox_new) +\n  tm_polygons() +\n  tm_shape(cities_sf) +\n  tm_dots(size = .25, col = \"red\") +\n  tm_text(text = \"Name\", auto.placement = T) +\n  tm_layout(title = \"Main Cities of\\nMadagascar\")\n\n\n\n\n\n\n\n#the city names are long so we have to \n# make a bigger window to fit them. This isn't part of the normal process\n#make an object with the current bounding box\nbbox_new &lt;- st_bbox(mdg)\n\n#calculate the x and y ranges of the bbox\nxrange &lt;- bbox_new$xmax - bbox_new$xmin # range of x values\nyrange &lt;- bbox_new$ymax - bbox_new$ymin # range of y values\n\n#provide the new values for the 4 corners of the bbox\n  bbox_new[1] &lt;- bbox_new[1] - (0.5 * xrange) # xmin - left\n  bbox_new[3] &lt;- bbox_new[3] + (0.5 * xrange) # xmax - right\n  bbox_new[2] &lt;- bbox_new[2] - (0.1 * yrange) # ymin - bottom\n  bbox_new[4] &lt;- bbox_new[4] + (0.1 * yrange) # ymax - top\n\n#convert the bbox to a sf collection (sfc)\nbbox_new &lt;- bbox_new |&gt;  # take the bounding box\n  st_as_sfc() # ... and make it a sf polygon\n\n\nggplot2::ggplot() +\n  geom_sf(data = mdg) +\n  geom_sf(data = cities_sf, color = \"red\") +\n  ggrepel::geom_text_repel(data = cities_sf\n               , aes(label = Name\n                     , geometry = geometry)\n               , stat = \"sf_coordinates\"\n               , min.segment.length = 0) +\n  coord_sf(xlim = st_coordinates(bbox_new)[c(1,2),1], # min & max of x values\n           ylim = st_coordinates(bbox_new)[c(2,3),2]) + # min & max of y values +\n  labs(title = \"Main Cities of\\nMadagascar\") +\n  theme_void()"
  },
  {
    "objectID": "Data exploration.html#final-touches",
    "href": "Data exploration.html#final-touches",
    "title": "Data exploration",
    "section": "Final touches",
    "text": "Final touches\nNow that we have a map with cities plotted (we achieved our goal!), we will add a few finishing touches and set the size of the city points to the population variable in the original dataset.\nAdditionally, tmap provides a simple interface to go from a static map to an interative map simply by changing tmap_mode(\"plot\") to tmap_mode(\"view\").\n\ntmapggplot2tmap interactive\n\n\n\n#the city names are long so we have to \n# make a bigger window to fit them. This isn't part of the normal process\n#make an object with the current bounding box\nbbox_new &lt;- st_bbox(mdg)\n\n#calculate the x and y ranges of the bbox\nxrange &lt;- bbox_new$xmax - bbox_new$xmin # range of x values\nyrange &lt;- bbox_new$ymax - bbox_new$ymin # range of y values\n\n#provide the new values for the 4 corners of the bbox\n  bbox_new[1] &lt;- bbox_new[1] - (0.7 * xrange) # xmin - left\n  bbox_new[3] &lt;- bbox_new[3] + (0.75 * xrange) # xmax - right\n  bbox_new[2] &lt;- bbox_new[2] - (0.1 * yrange) # ymin - bottom\n  bbox_new[4] &lt;- bbox_new[4] + (0.1 * yrange) # ymax - top\n\n#convert the bbox to a sf collection (sfc)\nbbox_new &lt;- bbox_new |&gt;  # take the bounding box ...\n  st_as_sfc() # ... and make it a sf polygon\ntmap_mode(\"plot\") +\n  tm_shape(mdg, bbox = bbox_new) +\n  tm_polygons() +\n  tm_shape(cities_sf) +\n  tm_dots(size = \"Population\", col = \"red\"\n          , legend.size.is.portrait = TRUE) +\n  tm_text(text = \"Name\", auto.placement = T\n          , along.lines = T) +\n  tm_scale_bar(position = c(\"left\", \"bottom\"), width = 0.15) +\n  tm_compass(type = \"4star\"\n             , position = c(\"right\", \"bottom\")\n             , size = 2) +\n  tm_layout(main.title = \"Main Cities of Madagascar\"                         , legend.outside = TRUE)\n\n\n\n\n\n\n\nggplot2::ggplot() +\n  geom_sf(data = mdg) +\n  geom_sf(data = cities_sf, aes(size = Population)\n          , color = \"red\") +\n  ggrepel::geom_text_repel(data = cities_sf\n               , aes(label = Name\n                     , geometry = geometry)\n               , stat = \"sf_coordinates\"\n               , min.segment.length = 0) +\n  coord_sf(xlim = st_coordinates(bbox_new)[c(1,2),1], # min & max of x values\n           ylim = st_coordinates(bbox_new)[c(2,3),2]) + # min & max of y values +\n  ggspatial::annotation_scale(location = \"bl\") +\n  ggspatial::annotation_north_arrow(location = \"br\"\n                                    , which_north = \"true\"\n                                    , size = 1)+\n  labs(title = \"Main Cities of Madagascar\") +\n  theme_void()\n\n\n\n\n\n\n\ntmap_mode(\"view\") +\n  tm_shape(mdg) +\n  tm_borders() +\n  tm_shape(cities_sf) +\n  tm_dots(size = \"Population\", col = \"red\"\n          , legend.size.is.portrait = TRUE) +\n  tm_text(text = \"Name\", auto.placement = T\n          , along.lines = T) +\n  tm_scale_bar(position = c(\"left\", \"bottom\"), width = 0.15) +\n  tm_compass(type = \"4star\"\n             , position = c(\"right\", \"bottom\")\n             , size = 2) +\n  tm_layout(main.title = \"Main Cities of Madagascar\"                         , legend.outside = TRUE)"
  },
  {
    "objectID": "Data exploration.html#additional-resources",
    "href": "Data exploration.html#additional-resources",
    "title": "Data exploration",
    "section": "Additional Resources",
    "text": "Additional Resources\nFor those interested in mapping in R (or QGIS) there are many free resources available online. A great starting point for R is the online text book, Geocomputation with R. If you would rather learn more in Python, Geocomputation with Python is a great resource."
  },
  {
    "objectID": "Data cleaning and preparation.html#instrument-design-and-scripting",
    "href": "Data cleaning and preparation.html#instrument-design-and-scripting",
    "title": "Data cleaning and preparation",
    "section": "Instrument Design and Scripting",
    "text": "Instrument Design and Scripting\n\nInstrument Design and Scripting Workflows\nIn the process of designing and scripting instruments for data collection, it is crucial to follow a systematic workflow that ensures accuracy, completeness, and ease of updating. Before going through this section it is important to consider several overarching points:\n\nClient preferences will likely dictate in what form instruments are shared.\nSpending the time to develop a thorough data dictionary from the onset is worth it!\nInvolve both HO and FO activity managers in instrument pre-testing whenever possible.\n\nBelow, we outline two workflows: the “Typical” Instrument Design and Scripting Workflow, and the “Proposed” Instrument Design and Scripting Workflow. The proposed workflow aims to address and improve upon the shortcomings observed in the typical workflow.\n“Typical” Instrument Design and Scripting Workflow The typical workflow consists of five main stages:\nIP/Client Document Review and Consultations:\n\nThis initial stage involves reviewing relevant documents provided by the implementing partners (IPs) or clients and conducting consultations to understand the requirements and context.\n\nInstrument Development in Word:\n\nBased on the gathered information, instruments are developed in Word documents. This step is critical but often leads to instruments that are difficult to update and manage, especially regarding data names and skip logic.\n\nClient/IP Feedback:\n\nThe draft instruments are then shared with the client or IP for feedback. This stage is essential for ensuring the instrument meets the client’s needs and expectations.\n\nUpdate Instruments in Word:\n\nFollowing feedback, the instruments are updated in Word. This iterative process can be time-consuming and prone to errors, particularly in managing complex logic and data structures. Script Instruments from Word into Data Collection Software:\nFinally, the instruments are scripted into the data collection software. This step often reveals issues not apparent in the Word documents, such as missing data names and ineffective skip logic, making the scripting process cumbersome. Typically, the client version of the instruments lacks unique data names, effective skip logic, and is challenging to update, leading to inefficiencies and potential errors in the data collection process.\n\nProposed Instrument Design and Scripting Workflow To overcome the limitations of the typical workflow, the proposed workflow introduces a more structured and efficient approach.\nIP/Client Document Review and Consultations:\n\nAs in the typical workflow, this stage involves a thorough review of documents and consultations to gather requirements.\n\nData Dictionary Development:\n\nInstead of directly developing instruments in Word, this stage focuses on creating a comprehensive data dictionary. The data dictionary includes unique data names, skip logic, and translations, providing a clear and structured framework for the instruments.\n\nClient/IP Feedback:\n\nThe data dictionary is shared with the client or IP for feedback, ensuring that all necessary elements are captured and agreed upon before developing the instruments. Update Data Dictionary:\n\nBased on the feedback, the data dictionary is updated. This step ensures that any changes or additions are systematically incorporated, maintaining the integrity and structure of the data. Script Instruments into Data Collection Software:\n\nWith a well-defined data dictionary, scripting the instruments into the data collection software becomes more straightforward and less error-prone. The unique data names and effective skip logic defined in the data dictionary ensure a smooth transition from design to implementation.\n\nBy adopting the proposed workflow, the instrument design and scripting process becomes more efficient and reliable. The use of a data dictionary ensures that all elements are clearly defined and managed, reducing the likelihood of errors and making the instruments easier to update and maintain. This approach not only improves the quality of the instruments but also enhances the overall efficiency of the data collection process."
  },
  {
    "objectID": "Data cleaning and preparation.html#common-scripting-issues",
    "href": "Data cleaning and preparation.html#common-scripting-issues",
    "title": "Data cleaning and preparation",
    "section": "Common Scripting Issues",
    "text": "Common Scripting Issues"
  }
]